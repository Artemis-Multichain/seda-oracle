This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-11-02T00:09:36.884Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.build/
  debug/
    docker-compose.yml
    docker-entrypoint.sh
    Dockerfile
    repopack-output.txt
  docker/
    docker-compose.yml
    Dockerfile
.devcontainer/
  devcontainer.json
  Dockerfile
.github/
  workflows/
    release.yml
    test.yml
helm/
  templates/
    _helpers.tpl
    configmap.yaml
    deployment.yaml
    secret.yaml
    service.yaml
  .helmignore
  Chart.yaml
  values.yaml
workspace/
  data-proxy/
    src/
      cli/
        utils/
          big.ts
          key-pair.ts
          private-key.ts
        init.ts
        register.ts
        run.ts
      status-plugin/
        index.ts
        status-context.ts
        status-plugin.ts
        types.ts
      testutils/
        mock-upstream.ts
      utils/
        create-headers.ts
        query-json.test.ts
        query-json.ts
        replace-params.test.ts
        replace-params.ts
        search-params.test.ts
        search-params.ts
        url.test.ts
        url.ts
      config-parser.test.ts
      config-parser.ts
      constants.ts
      index.ts
      logger.ts
      proxy-server.test.ts
      proxy-server.ts
    config.example.json
    package.json
  data-proxy-sdk/
    src/
      config.ts
      constants.ts
      data-proxy.test.ts
      data-proxy.ts
      index.ts
      latest-core-contract-address.ts
    package.json
.dockerignore
.env.example
.gitignore
.pre-commit-config.yaml
biome.json
bunfig.toml
Makefile
package.json
Procfile
README.md
test-setup.ts
tsconfig.json

================================================================
Repository Files
================================================================

================
File: .build/debug/docker-compose.yml
================
services:
  seda-data-proxy:
    build:
      context: ../..
      dockerfile: .build/docker/Dockerfile
      args:
        TARGET_ARCH: ${TARGET_ARCH:-bun-linux-arm64}
    ports:
      - "5384:5384"
    environment:
      API_KEY: ${API_KEY}
    volumes:
      - type: bind
        source: ../../config.json
        target: /app/config.json
        read_only: true
    networks:
      - proxy-network

networks:
  proxy-network:
    driver: bridge

================
File: .build/debug/docker-entrypoint.sh
================
#!/bin/bash

# Initialize flags for config.json and private key
CONFIG_EXISTS=false
PK_EXISTS=false

# Check if config.json exists
if [ -f /app/config.json ]; then
  echo "config.json detected"
  CONFIG_EXISTS=true
else
  echo "config.json not found"
fi

# Check if data-proxy-private-key.json exists
if [ -f /app/data-proxy-private-key.json ]; then
  echo "data-proxy-private-key.json detected"
  PK_EXISTS=true
elif [ -n "$SEDA_DATA_PROXY_PRIVATE_KEY" ]; then
  # If private key file does not exist, check if the private key is provided via environment variable
  echo "Private key provided via environment variable"
  echo "$SEDA_DATA_PROXY_PRIVATE_KEY" >/app/data-proxy-private-key.json
  PK_EXISTS=true
else
  echo "No private key provided"
fi

run_bun_command() {
  if ! bun "$@"; then
    echo "Failed to run: bun $*"
    exit 1
  fi
}

# Determine the command to run based on the presence of config.json and private key
if [ "$CONFIG_EXISTS" = true ] && [ "$PK_EXISTS" = true ]; then
  # Both config.json and private key are provided
  echo "Running with config and private key"
  RUN_CMD="start run --config /app/config.json --private-key-file /app/data-proxy-private-key.json"
elif [ "$CONFIG_EXISTS" = true ] && [ "$PK_EXISTS" = false ]; then
  # Only config.json is provided
  echo "Running with config only"
  run_bun_command start init
  RUN_CMD="start run --config /app/config.json"
else
  # Neither config.json nor private key is provided
  echo "Running with --disable-proof"
  run_bun_command start init
  RUN_CMD="start run --disable-proof"
fi

# Execute the final command
run_bun_command $RUN_CMD

================
File: .build/debug/Dockerfile
================
FROM oven/bun:alpine

WORKDIR /app

COPY . .

RUN bun install

# Expose the port the app runs on
EXPOSE 5384

# Entry script to handle conditional startup
COPY .build/debug/docker-entrypoint.sh ./docker-entrypoint.sh
RUN chmod +x ./docker-entrypoint.sh

ENTRYPOINT ["sh", "./docker-entrypoint.sh"]

================
File: .build/debug/repopack-output.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-30T15:19:01.298Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
docker-compose.yml
docker-entrypoint.sh
Dockerfile

================================================================
Repository Files
================================================================

================
File: docker-compose.yml
================
services:
  seda-data-proxy:
    build:
      context: ../..
      dockerfile: .build/docker/Dockerfile
    ports:
      - "5384:5384"
    # environment:
    #   # Provide the private key if available
    #   SEDA_DATA_PROXY_PRIVATE_KEY: ${SEDA_DATA_PROXY_PRIVATE_KEY}
    #
    # volumes:
    #   # Mount config.json if it exists in the host folder
    #   - ./config.json:/app/config.json:ro
    #   # Mount a data proxy private key file
    #   - ./data-proxy-private-key.json:/app/data-proxy-private-key.json:ro
    networks:
      - proxy-network

networks:
  proxy-network:
    driver: bridge

================
File: docker-entrypoint.sh
================
#!/bin/bash

# Initialize flags for config.json and private key
CONFIG_EXISTS=false
PK_EXISTS=false

# Check if config.json exists
if [ -f /app/config.json ]; then
  echo "config.json detected"
  CONFIG_EXISTS=true
else
  echo "config.json not found"
fi

# Check if data-proxy-private-key.json exists
if [ -f /app/data-proxy-private-key.json ]; then
  echo "data-proxy-private-key.json detected"
  PK_EXISTS=true
elif [ -n "$SEDA_DATA_PROXY_PRIVATE_KEY" ]; then
  # If private key file does not exist, check if the private key is provided via environment variable
  echo "Private key provided via environment variable"
  echo "$SEDA_DATA_PROXY_PRIVATE_KEY" >/app/data-proxy-private-key.json
  PK_EXISTS=true
else
  echo "No private key provided"
fi

run_bun_command() {
  if ! bun "$@"; then
    echo "Failed to run: bun $*"
    exit 1
  fi
}

# Determine the command to run based on the presence of config.json and private key
if [ "$CONFIG_EXISTS" = true ] && [ "$PK_EXISTS" = true ]; then
  # Both config.json and private key are provided
  echo "Running with config and private key"
  RUN_CMD="start run --config /app/config.json --private-key-file /app/data-proxy-private-key.json"
elif [ "$CONFIG_EXISTS" = true ] && [ "$PK_EXISTS" = false ]; then
  # Only config.json is provided
  echo "Running with config only"
  run_bun_command start init
  RUN_CMD="start run --config /app/config.json"
else
  # Neither config.json nor private key is provided
  echo "Running with --disable-proof"
  run_bun_command start init
  RUN_CMD="start run --disable-proof"
fi

# Execute the final command
run_bun_command $RUN_CMD

================
File: Dockerfile
================
FROM oven/bun:alpine

WORKDIR /app

COPY . .

RUN bun install

# Expose the port the app runs on
EXPOSE 5384

# Entry script to handle conditional startup
COPY .build/debug/docker-entrypoint.sh ./docker-entrypoint.sh
RUN chmod +x ./docker-entrypoint.sh

ENTRYPOINT ["sh", "./docker-entrypoint.sh"]

================
File: .build/docker/docker-compose.yml
================
services:
  seda-data-proxy:
    command: ['run', '--disable-proof']
    build:
      context: ../..
      dockerfile: .build/docker/Dockerfile
      args:
        TARGET_ARCH: ${TARGET_ARCH:-bun-linux-arm64}
    ports:
      - '5384:5384'
    environment:
      API_KEY: 'N97FlFhQkTy3OzibNZCZi4SPi8t61OZTW7ZJ9zNVFdEH7r1rAAb2ikjxwRaLBfR1'
    ### Provide the private key if available
    # environment:
    #   # Provide the private key if available
    #   SEDA_DATA_PROXY_PRIVATE_KEY: ${SEDA_DATA_PROXY_PRIVATE_KEY}
    #
    ### Provide the config file if available
    volumes:
      - type: bind
        source: ../../config.json
        target: /app/config.json
        read_only: true
        consistency: cached
      - type: bind
        source: ../../data-proxy-private-key.json
        target: /app/data-proxy-private-key.json
        read_only: true
        consistency: cached
    networks:
      - proxy-network

networks:
  proxy-network:
    driver: bridge

================
File: .build/docker/Dockerfile
================
FROM oven/bun:alpine

ARG TARGET_ARCH

WORKDIR /app

COPY . .

RUN bun install --frozen-lockfile

RUN bun build --compile --minify --sourcemap ./workspace/data-proxy/src/index.ts --outfile dataproxy

# Expose the port the app runs on
EXPOSE 5384

RUN chmod +x dataproxy

ENTRYPOINT ["./dataproxy"]
CMD ["run"]

================
File: .devcontainer/devcontainer.json
================
{
	"name": "seda-sdk",
	"build": {
		"dockerfile": "Dockerfile",
		"args": {
			"VARIANT": "1.1.26"
		}
	},
	"customizations": {
		"vscode": {
			"settings": {},
			"extensions": ["EditorConfig.EditorConfig"]
		}
	},
	"postCreateCommand": "bun install"
}

================
File: .devcontainer/Dockerfile
================
ARG VARIANT=latest
FROM oven/bun:${VARIANT}

RUN apt-get update
RUN chsh -s $(which bash) bun
RUN echo 'export PS1="\e[01;32m\u\e[m:\e[01;34m\w\e[m\$ "' >> /home/bun/.bashrc

USER bun

================
File: .github/workflows/release.yml
================
name: 🚀 Release

on:
  push:
    tags: ["*"]

permissions:
  contents: write
  packages: write

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: sedaprotocol/seda-data-proxy

jobs:
  build-and-push-amd64:
    name: 🐳 Build and Push Docker Image (amd64)
    runs-on: ubuntu-latest

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔐 Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: 🏗️ Build and push Docker image for amd64
        run: |
          docker build \
            --build-arg TARGET_ARCH=bun-linux-x64-modern \
            -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-x64-modern \
            -f .build/docker/Dockerfile \
            .
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-x64-modern

  build-and-push-arm64:
    name: 🐳 Build and Push Docker Image (arm64)
    runs-on: ubuntu-latest

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🛠️ Set up QEMU for ARM64
        uses: docker/setup-qemu-action@v3
        with:
          platforms: arm64

      - name: 🔐 Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: 🏗️ Build and push Docker image for arm64
        run: |
          docker build \
            --platform linux/arm64 \
            --build-arg TARGET_ARCH=bun-linux-arm64 \
            -t ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-arm64 \
            -f .build/docker/Dockerfile \
            .
          docker push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-arm64

  create-manifest:
    name: 📝 Create and Push Docker Manifest
    needs:
      - build-and-push-amd64
      - build-and-push-arm64
    runs-on: ubuntu-latest

    steps:
      - name: 🔐 Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: 📝 Create and Push Manifest
        run: |
          docker manifest create ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }} \
            --amend ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-x64-modern \
            --amend ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}-bun-linux-arm64
          docker manifest push ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}

  create-release:
    name: 📦 Create GitHub Release
    needs: create-manifest
    runs-on: ubuntu-latest

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 📝 Generate Changelog
        id: changelog
        uses: TriPSs/conventional-changelog-action@v5.3.0
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          output-file: false
          skip-commit: true
          skip-tag: true
          skip-git-pull: true
          git-push: false

      - name: 🎉 Create GitHub Release
        uses: ncipollo/release-action@v1
        with:
          allowUpdates: true
          generateReleaseNotes: true
          tag: ${{ github.ref_name }}
          name: ${{ github.ref_name }}
          body: |
            ${{ steps.changelog.outputs.changelog }}

            ## Docker Images
            The following Docker images were built and published to GHCR:

            - [ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }}-bun-linux-x64-modern](https://ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }}-bun-linux-x64-modern)
            - [ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }}-bun-linux-arm64](https://ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }}-bun-linux-arm64)
            - [ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }}](https://ghcr.io/sedaprotocol/seda-data-proxy:${{ github.ref_name }})
          token: ${{ secrets.GITHUB_TOKEN }}

================
File: .github/workflows/test.yml
================
name: 🧪 Test
on: [pull_request, push]

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4
      - uses: oven-sh/setup-bun@v2

      - name: 📦 Install dependencies
        run: bun install

      - name: 📝 Check formatting
        run: bun check-fmt

      - name: 🧪 Test project
        run: bun test

================
File: helm/templates/_helpers.tpl
================
{{/*
Generate a full name for the resources, optionally including the release name.
*/}}
{{- define "seda-data-proxy.fullname" -}}
{{- printf "%s-%s" .Release.Name .Chart.Name | trunc 63 | trimSuffix "-" -}}
{{- end }}

{{/*
Common labels
*/}}
{{- define "seda-data-proxy.labels" -}}
app.kubernetes.io/name: {{ include "seda-data-proxy.name" . }}
helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}
app.kubernetes.io/instance: {{ .Release.Name }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "seda-data-proxy.selectorLabels" -}}
app.kubernetes.io/name: {{ include "seda-data-proxy.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Template for the name of the application
*/}}
{{- define "seda-data-proxy.name" -}}
{{- .Chart.Name -}}
{{- end }}

================
File: helm/templates/configmap.yaml
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "seda-data-proxy.fullname" . }}-config
data:
  config.json: |
    {
      "routes": [
        {
          "path": "/*",
          "upstreamUrl": "https://swapi.dev/api/",
          "methods": ["GET"]
        }
      ]
    }

================
File: helm/templates/deployment.yaml
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "seda-data-proxy.fullname" . }}
  labels:
    {{- include "seda-data-proxy.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "seda-data-proxy.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "seda-data-proxy.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: {{ .Values.service.port }}
          env:
            {{- range .Values.envVars }}
            - name: {{ .name }}
              value: {{ .value }}
            {{- end }}
            - name: SEDA_DATA_PROXY_PRIVATE_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "seda-data-proxy.fullname" . }}-secrets
                  key: SEDA_DATA_PROXY_PRIVATE_KEY
          args: ["run", {{ .Values.sedaProxyFlags }} ]
          readinessProbe:
            httpGet:
              path: /status/health
              port: {{ .Values.service.port }}
          livenessProbe:
            httpGet:
              path: /status/health
              port: {{ .Values.service.port }}
          volumeMounts:
          - name: config-volume
            mountPath: /app/config.json
            subPath: config.json
      volumes:
        - name: config-volume
          configMap:
            name: {{ include "seda-data-proxy.fullname" . }}-config

================
File: helm/templates/secret.yaml
================
apiVersion: v1
kind: Secret
metadata:
  name: {{ include "seda-data-proxy.fullname" . }}-secrets
  labels:
    {{- include "seda-data-proxy.labels" . | nindent 4 }}
type: Opaque
data:
  SEDA_DATA_PROXY_PRIVATE_KEY: {{ .Values.secret.sedaDataProxyPrivateKey | b64enc | quote }}

================
File: helm/templates/service.yaml
================
apiVersion: v1
kind: Service
metadata:
  name: {{ include "seda-data-proxy.fullname" . }}
  labels:
    {{- include "seda-data-proxy.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: {{ .Values.service.port }}
  selector:
    {{- include "seda-data-proxy.selectorLabels" . | nindent 4 }}

================
File: helm/.helmignore
================
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*.orig
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/

================
File: helm/Chart.yaml
================
apiVersion: v2
name: seda-data-proxy
description: A Helm chart for Kubernetes to deploy the SEDA Data Proxy.
version: 0.1.0
appVersion: "1.0"

================
File: helm/values.yaml
================
imagePullSecrets: []

image:
  repository: ghcr.io/sedaprotocol/seda-data-proxy
  tag: "v0.0.3"
  pullPolicy: IfNotPresent
  pullSecrets: {}
  os: linux
  arch: amd64

service:
  type: ClusterIP
  port: 5384

replicaCount: 1

# export SEDA_PRIVATE_KEY=<KEY>
# helm install my-release ./chart-name --set secret.sedaDataProxyPrivateKey=$SEDA_PRIVATE_KEY
secret:
  sedaDataProxyPrivateKey: ""

# Uncomment for testing (itdisables request verification)
# sedaProxyFlags: "--disable-proof"

================
File: workspace/data-proxy/src/cli/utils/big.ts
================
import Big, { type BigSource } from "big.js";

export default Big;

Big.PE = 100000;

export const SEDA_EXPONENT = 18;

export function toDecimal(amount: BigSource, decimals: number): Big {
	const exponent = new Big(10).pow(decimals);
	return new Big(amount).div(exponent);
}

export function fromDecimal(amount: BigSource, decimals: number): Big {
	const exponent = new Big(10).pow(decimals);
	return new Big(amount).mul(exponent);
}

export function sedaToAseda(amount: BigSource): Big {
	return fromDecimal(amount, SEDA_EXPONENT);
}

export function asedaToSeda(amount: BigSource): Big {
	return toDecimal(amount, SEDA_EXPONENT);
}

================
File: workspace/data-proxy/src/cli/utils/key-pair.ts
================
import * as v from "valibot";

export const FileKeyPairSchema = v.object({
	pubkey: v.optional(v.string()),
	privkey: v.string(),
});

export type FileKeyPair = v.InferOutput<typeof FileKeyPairSchema>;

================
File: workspace/data-proxy/src/cli/utils/private-key.ts
================
import { readFile } from "node:fs/promises";
import { tryAsync, tryParseSync, trySync } from "@seda-protocol/utils";
import { Result } from "true-myth";
import {
	DEFAULT_PRIVATE_KEY_JSON_FILE_NAME,
	PRIVATE_KEY,
} from "../../constants";
import { FileKeyPairSchema } from "./key-pair";

async function readPrivateKeyFile(
	path: string,
): Promise<Result<Buffer, unknown>> {
	const privateKeyFile = await tryAsync(async () => readFile(path));

	return privateKeyFile;
}

export async function loadPrivateKey(
	privateKeyFilePath?: string,
): Promise<Result<Buffer, string>> {
	if (!privateKeyFilePath && PRIVATE_KEY) {
		return Result.ok(Buffer.from(PRIVATE_KEY, "hex"));
	}

	const privateKeyFile = await readPrivateKeyFile(
		privateKeyFilePath ?? DEFAULT_PRIVATE_KEY_JSON_FILE_NAME,
	);

	if (privateKeyFile.isErr) {
		return Result.err(
			`Failed to read private key file ${privateKeyFilePath}: ${privateKeyFile.error}`,
		);
	}

	const privateKeyFileObject = trySync(() =>
		JSON.parse(privateKeyFile.value.toString()),
	);

	if (privateKeyFileObject.isErr) {
		return Result.err(
			`Failed to read private key file as JSON: ${privateKeyFileObject.error}`,
		);
	}

	const parsedPrivateKeyFile = tryParseSync(
		FileKeyPairSchema,
		privateKeyFileObject.value,
	);

	if (parsedPrivateKeyFile.isErr) {
		let resultError = "";

		for (const error of parsedPrivateKeyFile.error) {
			resultError += `${error.message} on config property "${error.path?.[0].key}" \n`;
		}

		return Result.err(`Failed to parse private key file: \n ${resultError}`);
	}

	return Result.ok(Buffer.from(parsedPrivateKeyFile.value.privkey, "hex"));
}

================
File: workspace/data-proxy/src/cli/init.ts
================
import { randomBytes } from 'node:crypto';
import { exists, writeFile } from 'node:fs/promises';
import { Command } from '../../../../node_modules/@commander-js/extra-typings';
import { Secp256k1 } from '@cosmjs/crypto';
import { tryAsync } from '@seda-protocol/utils';
import type { Config } from '../config-parser';
import { DEFAULT_PRIVATE_KEY_JSON_FILE_NAME } from '../constants';
import type { FileKeyPair } from './utils/key-pair';

async function outputJson(
  content: string,
  filePath: string,
  printOnly = false
) {
  if (printOnly) {
    console.log(`Content for ${filePath}:`);
    console.log(content);
    return;
  }

  const writeResult = await tryAsync(async () => writeFile(filePath, content));

  if (writeResult.isErr) {
    console.error(`Writing file to ${filePath} errored: ${writeResult.error}`);
    process.exit(1);
  }

  console.info(`Written to ${filePath}`);
}

export const initCommand = new Command('init')
  .description('Initializes a config.json file and generates a private key')
  .option(
    '-pkf, --private-key-file <string>',
    'Path where to create the private key json',
    DEFAULT_PRIVATE_KEY_JSON_FILE_NAME
  )
  .option('-c, --config <string>', 'Path to config.json', './config.json')
  .option('--print', 'Print the content instead of writing it')
  .action(async (args) => {
    if (!(await exists(args.privateKeyFile))) {
      const privateKeyBuff = randomBytes(32);
      const keyPair = await Secp256k1.makeKeypair(privateKeyBuff);
      const keyPairJson: FileKeyPair = {
        pubkey: Buffer.from(Secp256k1.compressPubkey(keyPair.pubkey)).toString(
          'hex'
        ),
        privkey: Buffer.from(keyPair.privkey).toString('hex'),
      };

      await outputJson(
        JSON.stringify(keyPairJson, null, 2),
        args.privateKeyFile,
        args.print
      );
    } else {
      console.warn(
        `${args.privateKeyFile} already exists skipping creation of private key`
      );
    }

    if (!(await exists(args.config))) {
      const config = {
        routeGroup: 'proxy',
        routes: [
          {
            path: '/chatgpt',
            method: ['GET', 'OPTIONS', 'POST'],
            upstreamUrl: 'https://www.artemysai.xyz/api/generatePrompt',
            stripPrefix: false,
            forwardPrefix: false,
            responseHeaders: {
              'Access-Control-Allow-Origin': 'http://127.0.0.1:5500',
              'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
              'Access-Control-Allow-Headers': '*',
              'Access-Control-Allow-Credentials': 'true',
            },
            headers: {
              'x-api-key': '{$API_KEY}',
            },
          },
        ],
      };

      await outputJson(
        JSON.stringify(config, null, 2),
        args.config,
        args.print
      );
    } else {
      console.warn(`${args.config} already exists skipping creation of config`);
    }

    console.info('Done');
  });

================
File: workspace/data-proxy/src/cli/register.ts
================
import { Command } from '../../../../node_modules/@commander-js/extra-typings';
import { Keccak256 } from '@cosmjs/crypto';
import { Environment } from '@seda-protocol/data-proxy-sdk';
import { defaultConfig } from '@seda-protocol/data-proxy-sdk/src/config';
import { trySync } from '@seda-protocol/utils';
import { ecdsaSign, publicKeyCreate } from 'secp256k1';
import { Maybe } from 'true-myth';
import { DEFAULT_ENVIRONMENT, PRIVATE_KEY_ENV_KEY } from '../constants';
import { sedaToAseda } from './utils/big';
import { loadPrivateKey } from './utils/private-key';

export const registerCommand = new Command('register')
  .description('Register the Data Proxy node on the SEDA chain')
  .argument(
    '<payout-address>',
    'SEDA chain address to payout for completing requests'
  )
  .argument('<fee>', 'Fee amount per request in SEDA')
  .option(
    '-pkf, --private-key-file <string>',
    'Path where to create the private key json'
  )
  .option(
    '-n, --network <network>',
    'The SEDA network to chose',
    DEFAULT_ENVIRONMENT
  )
  .option(
    '--memo <string>',
    'A custom note to attach to this Data Proxy registration'
  )
  .action(async (payoutAddress, fee, options) => {
    const network = Maybe.of(defaultConfig[options.network as Environment]);

    if (network.isNothing) {
      console.error(
        `Given network ${options.network} does not exist, please select ${Environment.Devnet}, ${Environment.Testnet} or ${Environment.Mainnet}`
      );
      process.exit(1);
    }

    const privateKey = await loadPrivateKey(options.privateKeyFile);

    if (privateKey.isErr) {
      console.error(privateKey.error);
      console.error(
        `Please make sure either the environment variable ${PRIVATE_KEY_ENV_KEY} is set or you pass in the -pkf argument`
      );
      process.exit(1);
    }

    // TODO: Validate if seda address is valid
    const aSedaAmount = trySync(() => sedaToAseda(fee)).map(
      (amount) => `${amount}aseda`
    );

    if (aSedaAmount.isErr) {
      console.error(`${fee} is not a valid number`);
      process.exit(1);
    }

    const memo = Maybe.of(options.memo).unwrapOr('');
    const hasher = new Keccak256(Buffer.from(aSedaAmount.value));

    hasher.update(Buffer.from(payoutAddress));
    hasher.update(Buffer.from(memo));
    const hash = Buffer.from(hasher.digest());

    const signatureRaw = ecdsaSign(hash, privateKey.value);
    const signature = Buffer.from(signatureRaw.signature);
    const publicKey = Buffer.from(publicKeyCreate(privateKey.value, true));

    const url = new URL('/data-proxy/register', network.value.explorerUrl);
    url.searchParams.append('fee', aSedaAmount.value);
    url.searchParams.append('payoutAddress', payoutAddress);
    url.searchParams.append('publicKey', publicKey.toString('hex'));
    url.searchParams.append('signature', signature.toString('hex'));
    url.searchParams.append('recoveryId', signatureRaw.recid.toString());
    url.searchParams.append('memo', memo);

    console.info(`Fee amount: \t\t${fee} SEDA (${aSedaAmount.value})`);
    console.info(`Payout address: \t${payoutAddress}`);
    console.info(`Signed hash: \t\t${hash.toString('hex')}`);
    console.info(`Public key: \t\t${publicKey.toString('hex')}`);
    console.info(`Signature: \t\t${signature.toString('hex')}`);
    console.info(`Signature recovery id: \t${signatureRaw.recid}`);
    console.info('');
    console.info(`Submit your transaction on: \n${url.toString()}`);
  });

================
File: workspace/data-proxy/src/cli/run.ts
================
import { readFile } from 'node:fs/promises';
import { Command } from '../../../../node_modules/@commander-js/extra-typings';
import { DataProxy, Environment } from '@seda-protocol/data-proxy-sdk';
import { defaultConfig } from '@seda-protocol/data-proxy-sdk/src/config';
import { tryAsync, trySync } from '@seda-protocol/utils';
import { Maybe } from 'true-myth';
import { parseConfig } from '../config-parser';
import {
  DEFAULT_ENVIRONMENT,
  DEFAULT_PRIVATE_KEY_JSON_FILE_NAME,
  PRIVATE_KEY_ENV_KEY,
  SERVER_PORT,
} from '../constants';
import logger from '../logger';
import { startProxyServer } from '../proxy-server';
import { loadPrivateKey } from './utils/private-key';

export const runCommand = new Command('run')
  .description('Run the Data Proxy node')
  .option('-c, --config <string>', 'Path to config.json', './config.json')
  .option('-p, --port <number>', 'Port to run the server on', SERVER_PORT)
  .option(
    '-pkf, --private-key-file <string>',
    `Path where to find the private key json (Defaults to either env variable $${PRIVATE_KEY_ENV_KEY} or ${DEFAULT_PRIVATE_KEY_JSON_FILE_NAME})`
  )
  .option(
    '-n, --network <network>',
    'The SEDA network to chose',
    DEFAULT_ENVIRONMENT
  )
  .option(
    '-dp, --disable-proof',
    'Disables proofing mechanism, useful for debugging',
    false
  )
  .option(
    '-cca, --core-contract-address <string>',
    'Optional setting of the core contract address, fetches it automatically by default'
  )
  .option('-r, --rpc <rpc-url>', 'Optional RPC URL to the SEDA network')
  .action(async (options) => {
    const network = Maybe.of(defaultConfig[options.network as Environment]);

    if (network.isNothing) {
      console.error(
        `Given network ${options.network} does not exist, please select ${Environment.Devnet}, ${Environment.Testnet} or ${Environment.Mainnet}`
      );
      process.exit(1);
    }

    const privateKey = await loadPrivateKey(options.privateKeyFile);

    if (privateKey.isErr) {
      console.error(privateKey.error);
      console.error(
        `Please make sure either the environment variable ${PRIVATE_KEY_ENV_KEY} is set or you pass in the -pkf argument`
      );
      process.exit(1);
    }

    const configFile = await tryAsync(async () => readFile(options.config));
    if (configFile.isErr) {
      console.error(`Failed to read config: ${configFile.error}`);
      process.exit(1);
    }

    const parsedConfig = trySync(() => JSON.parse(configFile.value.toString()));
    if (parsedConfig.isErr) {
      console.error(`Parsing config failed: ${parsedConfig.error}`);
      process.exit(1);
    }

    const config = parseConfig(parsedConfig.value);
    if (config.isErr) {
      console.error(`Invalid config: ${config.error}`);
      process.exit(1);
    }

    logger.info(`Environment: "${options.network}" will be used`);

    const dataProxy = new DataProxy(options.network as Environment, {
      privateKey: privateKey.value,
      rpcUrl: options.rpc,
      coreContract: options.coreContractAddress,
    });

    if (options.disableProof) {
      logger.warn(
        'Data Proxy will run without checking proofs, this is for development and testing only. Do not use in production'
      );
    }

    startProxyServer(config.value, dataProxy, {
      port: Number(options.port ?? SERVER_PORT),
      disableProof: options.disableProof,
    });
  });

================
File: workspace/data-proxy/src/status-plugin/index.ts
================
export { statusPlugin } from "./status-plugin";
export { StatusContext } from "./status-context";

================
File: workspace/data-proxy/src/status-plugin/status-context.ts
================
import { formatISODuration, intervalToDuration } from "date-fns";
import type { Context } from "./types";

export class StatusContext implements Context {
	readonly startedAt = new Date();
	private requests = 0;
	private errors = 0;

	constructor(private publicKey: string) {
		this.publicKey = publicKey;
	}

	incrementRequests() {
		this.requests++;
	}

	incrementErrors() {
		this.errors++;
	}

	getPublicKey() {
		return this.publicKey;
	}

	getMetrics() {
		return {
			uptime: formatISODuration(
				intervalToDuration({ start: this.startedAt, end: Date.now() }),
			),
			requests: this.requests,
			errors: this.errors,
		};
	}
}

================
File: workspace/data-proxy/src/status-plugin/status-plugin.ts
================
import Elysia from "elysia";
import type { Config } from "../config-parser";
import type { Context } from "./types";

export function statusPlugin(
	context: Context,
	options: Config["statusEndpoints"],
) {
	return (app: Elysia) => {
		const plugin = new Elysia({
			name: "status",
		});

		plugin.group(options.root, (group) => {
			if (options.apiKey) {
				const { header, secret } = options.apiKey;
				group.onBeforeHandle(({ request }) => {
					const apiKey = request.headers.get(header);
					if (apiKey !== secret) {
						return new Response("Unauthorized", { status: 401 });
					}
				});
			}

			group.get("health", () => {
				return Response.json({
					status: "healthy",
					metrics: context.getMetrics(),
				});
			});

			group.get("pubkey", () => {
				return Response.json({
					pubKey: context.getPublicKey(),
				});
			});

			return group;
		});

		return app.use(plugin);
	};
}

================
File: workspace/data-proxy/src/status-plugin/types.ts
================
export interface Context {
	getPublicKey(): string;

	getMetrics(): {
		uptime: string;
		requests: number;
		errors: number;
	};
}

================
File: workspace/data-proxy/src/testutils/mock-upstream.ts
================
import { http, type HttpResponseResolver, passthrough } from "msw";
import { setupServer } from "msw/node";
export { HttpResponse } from "msw";

const TEST_UPSTREAM_BASE = "https://proxy-upstream.com";

// TODO: either reuse a single proxy entry or figure out why stopping a server doesn't actually stop it.
// When calling `await proxy.stop()` after a test and spinning up a new proxy in the next test the first server still
// receives the request.
// https://github.com/sedaprotocol/seda-data-proxy/issues/11
let TEST_LOCAL_PROXY_PORT = 9000;
const TEST_LOCAL_PROXY_BASE = "http://localhost";

const handlers = [
	// Don't touch requests that go to the data proxy
	http.all(`${TEST_LOCAL_PROXY_BASE}*`, () => {
		return passthrough();
	}),
];

export const server = setupServer(...handlers);

type Method = keyof typeof http;

export function registerHandler(
	method: Method,
	path: string,
	resolver: HttpResponseResolver,
): {
	upstreamUrl: string;
	proxyUrl: string;
	path: string;
	port: number;
} {
	const upstreamUrl = `${TEST_UPSTREAM_BASE}${path}`;
	server.use(http[method](upstreamUrl, resolver));

	return {
		upstreamUrl,
		proxyUrl: `${TEST_LOCAL_PROXY_BASE}:${TEST_LOCAL_PROXY_PORT}${path}`,
		path,
		port: TEST_LOCAL_PROXY_PORT++,
	};
}

================
File: workspace/data-proxy/src/utils/create-headers.ts
================
import { constants, type SignedData } from "@seda-protocol/data-proxy-sdk";

export function createDefaultResponseHeaders() {
	const headers = new Headers();
	headers.append("content-type", "application/json");

	return headers;
}

export function createSignedResponseHeaders(
	signature: SignedData,
	headers = new Headers(),
) {
	headers.append(constants.SIGNATURE_HEADER_KEY, signature.signature);
	headers.append(constants.PUBLIC_KEY_HEADER_KEY, signature.publicKey);
	headers.append(constants.SIGNATURE_VERSION_HEADER_KEY, signature.version);

	return headers;
}

================
File: workspace/data-proxy/src/utils/query-json.test.ts
================
import { describe, expect, it } from "bun:test";
import { queryJson } from "./query-json";

describe("queryJson", () => {
	it("should be able to find a nested variable inside a JSON object", () => {
		const result = queryJson(
			JSON.stringify({ a: { b: { c: "ok" } } }),
			"$.a.b.c",
		);

		expect(result).toBeOkResult("ok");
	});

	it("should return an error when the variable was not found", () => {
		const result = queryJson(
			JSON.stringify({ a: { b: { c: "ok" } } }),
			"$.a.b.b",
		);

		expect(result).toBeErrResult("Quering JSON with $.a.b.b returned null");
	});
});

================
File: workspace/data-proxy/src/utils/query-json.ts
================
import { trySync } from "@seda-protocol/utils";
import { JSONPath } from "jsonpath-plus";
import { Result } from "true-myth";

export function queryJson(
	input: string | object,
	path: string,
): Result<unknown, string> {
	const jsonData: Result<object, unknown> =
		typeof input === "string"
			? trySync(() => JSON.parse(input))
			: Result.ok(input);

	if (jsonData.isErr) {
		return Result.err(`Parsing as JSON failed: ${jsonData.error}`);
	}

	const data = trySync(() => JSONPath({ path, json: jsonData.value }));

	if (data.isErr) {
		return Result.err(`Could not query JSON: ${data.error}`);
	}

	if (!data.value.length) {
		return Result.err(`Quering JSON with ${path} returned null`);
	}

	return Result.ok(data.value[0]);
}

================
File: workspace/data-proxy/src/utils/replace-params.test.ts
================
import { describe, expect, it } from "bun:test";
import { replaceParams } from "./replace-params";

describe("replaceParams", () => {
	it("should set the parameters for a url", () => {
		const result = replaceParams("price/{:coinA}/{:coinB}", {
			coinA: "eth",
			coinB: "usd",
		});

		expect(result).toBe("price/eth/usd");
	});

	it("should set the parameter for a url multiple times", () => {
		const result = replaceParams("price/{:coinA}/{:coinB}/{:coinA}", {
			coinA: "eth",
			coinB: "usd",
		});

		expect(result).toBe("price/eth/usd/eth");
	});

	it("should set the parameter for a url when using query params", () => {
		const result = replaceParams("price/{:coinA}/{:coinB}?myparam={:coinA}", {
			coinA: "eth",
			coinB: "usd",
		});

		expect(result).toBe("price/eth/usd?myparam=eth");
	});

	it("should set env variables", () => {
		process.env.MY_ENV_VARIABLE = "test";
		const result = replaceParams(
			"price/{:coinA}/{:coinB}?myparam={$MY_ENV_VARIABLE}",
			{
				coinA: "eth",
				coinB: "usd",
			},
		);
		process.env.MY_ENV_VARIABLE = undefined;

		expect(result).toBe("price/eth/usd?myparam=test");
	});
});

================
File: workspace/data-proxy/src/utils/replace-params.ts
================
export function replaceParams(
	input: string,
	params: Record<string, string> | undefined,
): string {
	let result = input;

	if (params) {
		for (const [key, value] of Object.entries(params)) {
			if (key === "*") {
				// Special use case where they do not use : for *
				result = result.replaceAll(`{${key}}`, value);
			}

			result = result.replaceAll(`{:${key}}`, value);
		}
	}

	// Allow replacement of {$ENV_VARIABLE} in case data providers want to safely store their API keys
	const envVariablesRegex = new RegExp(/{(\$[^}]+)}/g, "g");
	const envMatches = result.matchAll(envVariablesRegex);

	for (const match of envMatches) {
		const envKey = match[1].replace("$", "");

		// TODO: This should be checked at config parse level
		const envVar = process.env[envKey] ?? "";
		result = result.replaceAll(match[0], envVar);
	}

	return result;
}

================
File: workspace/data-proxy/src/utils/search-params.test.ts
================
import { describe, expect, it } from "bun:test";
import { mergeUrlSearchParams } from "./search-params";

describe("mergeUrlSearchParams", () => {
	it("should be able to merge two URLSearchParams together", () => {
		const a = new URLSearchParams({
			"1": "one",
			"2": "two",
		});

		const b = new URLSearchParams({
			"3": "three",
		});

		const result = mergeUrlSearchParams(a, b);
		const expected = new URLSearchParams({
			"1": "one",
			"2": "two",
			"3": "three",
		});

		expect(result.toString()).toBe(expected.toString());
	});

	it("should keep both a params and b params", () => {
		const a = new URLSearchParams({
			"1": "one",
			"2": "two",
		});

		const b = new URLSearchParams({
			"2": "test",
		});

		const result = mergeUrlSearchParams(a, b);
		const expected = new URLSearchParams({
			"1": "one",
			"2": "two",
		});

		expected.append("2", "test");

		expect(result.toString()).toBe(expected.toString());
	});
});

================
File: workspace/data-proxy/src/utils/search-params.ts
================
export function createUrlSearchParams(
	queryParams: Record<string, string | undefined>,
): URLSearchParams {
	const result = new URLSearchParams();

	for (const [key, value] of Object.entries(queryParams)) {
		result.append(key, value ?? "");
	}

	return result;
}

export function mergeUrlSearchParams(a: URLSearchParams, b: URLSearchParams) {
	const result = new URLSearchParams(a);
	b.forEach((value, key) => result.append(key, value));

	return result;
}

================
File: workspace/data-proxy/src/utils/url.test.ts
================
import { describe, expect, it } from "bun:test";
import { createUrlSearchParams } from "./search-params";
import { injectSearchParamsInUrl } from "./url";

describe("url", () => {
	it("should fill in query params on a target url", () => {
		const targetUrl = "http://example.com?one=1";
		const injection = createUrlSearchParams({
			two: "2",
		});

		const result = injectSearchParamsInUrl(targetUrl, injection);
		expect(result.toString()).toBe("http://example.com/?two=2&one=1");
	});
});

================
File: workspace/data-proxy/src/utils/url.ts
================
import { mergeUrlSearchParams } from "./search-params";

export function injectSearchParamsInUrl(
	targetUrl: string,
	searchParams: URLSearchParams,
): URL {
	const target = new URL(targetUrl);
	const finalSearchParams = mergeUrlSearchParams(
		searchParams,
		target.searchParams,
	);
	target.search = finalSearchParams.toString();

	return target;
}

================
File: workspace/data-proxy/src/config-parser.test.ts
================
import { describe, expect, it } from "bun:test";
import { assertIsOkResult } from "@seda-protocol/utils/testing";
import { parseConfig } from "./config-parser";

describe("parseConfig", () => {
	it("should check if route parameters are correctly used in the url", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com?myCoin={:coinA}&coinYo={:coinA}",
					jsonPath: "$.coin[0].{:coinB}",
				},
			],
		});

		expect(result).toBeOkResult();
	});

	it("should check if route parameters are using env variables and if they exist", async () => {
		process.env.MY_SECRET = "shhh";

		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com?myCoin={:coinA}&coinYo={:coinA}",
					jsonPath: "$.coin[0].{:coinB}",
					headers: {
						"x-secret": "api_key_{$MY_SECRET}",
					},
				},
			],
		});

		assertIsOkResult(result);
		expect(result.value.routes[0].headers).toEqual({
			"x-secret": "api_key_shhh",
		});
		process.env.MY_SECRET = undefined;
	});

	it("should error when a path arg has been used in the headers but was not set in the path", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com?myCoin={:coinA}&coinYo={:coinA}",
					jsonPath: "$.coin[0].{:coinB}",
					headers: {
						"x-secret": "{:ccccc}",
					},
				},
			],
		});

		expect(result).toBeErrResult(
			"Header x-secret required :ccccc but was not given in route /:coinA/:coinB",
		);
	});

	it("should check if route parameters are using env variables and if they exist", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com?myCoin={:coinA}&coinYo={:coinA}",
					jsonPath: "$.coin[0].{:coinB}",
					headers: {
						"x-secret": "{$NO_SECRET}",
					},
				},
			],
		});

		expect(result).toBeErrResult(
			"Header x-secret required NO_SECRET but was not available in the environment",
		);
	});

	it("should check if * is used in upstream url but not in path", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com/{*}",
				},
			],
		});

		expect(result).toBeErrResult(
			"UpstreamUrl: aaaaaa.com/{*} required {*} but path did not end with * (/:coinA/:coinB)",
		);
	});

	it("should check if * is used in upstream url but does not end with {*}", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/:coinB",
					upstreamUrl: "aaaaaa.com/{*}/something",
				},
			],
		});

		expect(result).toBeErrResult(
			"UpstreamUrl: aaaaaa.com/{*}/something uses {*} but was not at the end of the URL",
		);
	});

	it("should allow * paths", async () => {
		const result = parseConfig({
			routes: [
				{
					path: "/:coinA/*",
					upstreamUrl: "aaaaaa.com/{*}",
				},
			],
		});

		expect(result).toBeOkResult();
	});
});

================
File: workspace/data-proxy/src/config-parser.ts
================
import type { HTTPMethod } from "elysia";
import { Result } from "true-myth";
import * as v from "valibot";
import { DEFAULT_HTTP_METHODS, DEFAULT_PROXY_ROUTE_GROUP } from "./constants";
import { replaceParams } from "./utils/replace-params";

const HttpMethodSchema = v.union([v.string(), v.array(v.string())]);

const RouteSchema = v.object({
	path: v.string(),
	upstreamUrl: v.string(),
	method: v.optional(HttpMethodSchema, DEFAULT_HTTP_METHODS),
	jsonPath: v.optional(v.pipe(v.string(), v.startsWith("$"))),
	forwardResponseHeaders: v.pipe(
		v.optional(v.array(v.string()), []),
		v.transform((methods) => {
			return new Set(methods.map((method) => method.toLowerCase()));
		}),
	),
	headers: v.optional(v.record(v.string(), v.string()), {}),
});

const ConfigSchema = v.object({
	routeGroup: v.optional(v.string(), DEFAULT_PROXY_ROUTE_GROUP),
	routes: v.array(RouteSchema),
	statusEndpoints: v.optional(
		v.object({
			root: v.string(),
			apiKey: v.optional(
				v.object({
					header: v.string(),
					secret: v.string(),
				}),
			),
		}),
		{
			root: "status",
		},
	),
});

export type Route = v.InferOutput<typeof RouteSchema>;
export type Config = v.InferOutput<typeof ConfigSchema>;

export function getHttpMethods(
	configuredMethod: Route["method"],
): HTTPMethod[] {
	if (!configuredMethod) return DEFAULT_HTTP_METHODS;
	if (Array.isArray(configuredMethod)) return configuredMethod;

	return [configuredMethod];
}

const pathRegex = new RegExp(/{(:[^}]+)}/g, "g");
const envVariablesRegex = new RegExp(/{(\$[^}]+)}/g, "g");

export function parseConfig(input: unknown): Result<Config, string> {
	const config = v.parse(ConfigSchema, input);

	if (config.statusEndpoints.apiKey) {
		const statusApiSecretEnvMatches =
			config.statusEndpoints.apiKey.secret.matchAll(envVariablesRegex);

		for (const match of statusApiSecretEnvMatches) {
			const envKey = match[1].replace("$", "");
			const envVariable = process.env[envKey];

			if (!envVariable) {
				return Result.err(
					`Status endpoint API key secret required ${envKey} but was not available in the environment`,
				);
			}

			config.statusEndpoints.apiKey.secret = replaceParams(
				config.statusEndpoints.apiKey.secret,
				{},
			);
		}
	}

	for (const route of config.routes) {
		const urlMatches = route.upstreamUrl.matchAll(pathRegex);

		// Content type should always be forwarded to the client
		route.forwardResponseHeaders.add("content-type");

		if (route.upstreamUrl.includes("{*}")) {
			if (!route.upstreamUrl.endsWith("{*}")) {
				return Result.err(
					`UpstreamUrl: ${route.upstreamUrl} uses {*} but was not at the end of the URL`,
				);
			}

			if (!route.path.endsWith("*")) {
				return Result.err(
					`UpstreamUrl: ${route.upstreamUrl} required {*} but path did not end with * (${route.path})`,
				);
			}
		}

		// Check if any variables on the url are not available in the route
		for (const match of urlMatches) {
			if (!route.path.includes(match[1])) {
				return Result.err(
					`url required ${match[1]} but was not given in route ${route.path}`,
				);
			}
		}

		// Check if the json path is using variables that is not in the route
		const jsonPathMatches = route.jsonPath?.matchAll(pathRegex) ?? [];

		// Check if any variables on the url are not available in the route
		for (const match of jsonPathMatches) {
			if (!route.path.includes(match[1])) {
				return Result.err(
					`jsonPath required ${match[1]} but was not given in route ${route.path}`,
				);
			}
		}

		for (const [headerKey, headerValue] of Object.entries(route.headers)) {
			const headerValuePathMatches = headerValue.matchAll(pathRegex);

			for (const match of headerValuePathMatches) {
				if (!route.path.includes(match[1])) {
					return Result.err(
						`Header ${headerKey} required ${match[1]} but was not given in route ${route.path}`,
					);
				}
			}

			const headerValueEnvMatches = headerValue.matchAll(envVariablesRegex);

			for (const match of headerValueEnvMatches) {
				const envKey = match[1].replace("$", "");
				const envVariable = process.env[envKey];

				if (!envVariable) {
					return Result.err(
						`Header ${headerKey} required ${envKey} but was not available in the environment`,
					);
				}

				route.headers[headerKey] = replaceParams(route.headers[headerKey], {});
			}
		}
	}

	return Result.ok(config);
}

================
File: workspace/data-proxy/src/constants.ts
================
import { Environment } from "@seda-protocol/data-proxy-sdk";
import type { HTTPMethod } from "elysia";

// Server constants
export const SERVER_PORT = process.env.SERVER_PORT ?? "5384";
export const LOG_LEVEL = process.env.LOG_LEVEL ?? "info";
export const LOG_FILE_DIR = process.env.LOG_FILE_DIR ?? "";

// Environment constants
export const DEFAULT_ENVIRONMENT: Environment =
	(process.env.SEDA_ENV as Environment) ?? Environment.Devnet;

// App constants
export const JSON_PATH_HEADER_KEY = "x-seda-json-path";

export const PRIVATE_KEY_ENV_KEY = "SEDA_DATA_PROXY_PRIVATE_KEY";
export const PRIVATE_KEY = process.env[PRIVATE_KEY_ENV_KEY];
export const DEFAULT_PRIVATE_KEY_JSON_FILE_NAME =
	"./data-proxy-private-key.json";

// Where all the proxy routes go to (For example /proxy/CONFIGURED_ROUTE_HERE)
export const DEFAULT_PROXY_ROUTE_GROUP = "proxy";
// Default http methods set when no method is provided in the config
export const DEFAULT_HTTP_METHODS: HTTPMethod[] = [
	"GET",
	"PATCH",
	"POST",
	"PUT",
	"DELETE",
	"OPTIONS",
	"HEAD",
];

================
File: workspace/data-proxy/src/index.ts
================
#!/usr/bin/env node
import { Command } from '../../../node_modules/@commander-js/extra-typings';
import { version } from '../package.json';
import { initCommand } from './cli/init';
import { registerCommand } from './cli/register';
import { runCommand } from './cli/run';

const program = new Command()
  .description('SEDA Data Proxy CLI')
  .version(version)
  .addHelpText('after', '\r')
  .addCommand(runCommand)
  .addCommand(initCommand)
  .addCommand(registerCommand)
  .helpOption(undefined, 'Display this help');

program.parse(process.argv);

================
File: workspace/data-proxy/src/logger.ts
================
import { createLogger, format, type transport, transports } from "winston";
import "winston-daily-rotate-file";
import { Maybe } from "true-myth";
import { LOG_FILE_DIR, LOG_LEVEL } from "./constants";

const logFormat = format.printf((info) => {
	const requestId = Maybe.of(info.metadata?.requestId).mapOr(" ", (t) => {
		return ` [${cyan(t)}] `;
	});
	const logMsg = `${info.timestamp}${requestId}${info.level}`;

	return Maybe.of(info.metadata?.error).mapOr(
		`${logMsg}: ${info.message}`,
		(err) => `${logMsg}: ${info.message} ${err}`,
	);
});

const destinations: transport[] = [
	new transports.Console({
		format: format.combine(format.colorize(), logFormat),
	}),
];

if (LOG_FILE_DIR) {
	destinations.push(
		new transports.DailyRotateFile({
			filename: "data-proxy-%DATE%.log",
			dirname: LOG_FILE_DIR,
			format: format.json(),
			datePattern: "YYYY-MM-DD-HH",
			maxFiles: "14d",
		}),
	);
}

const logger = createLogger({
	level: LOG_LEVEL,
	format: format.combine(
		format.timestamp({ format: "YYYY-MM-DD HH:mm:ss.SSS" }),
		format.metadata({
			fillExcept: ["message", "level", "timestamp", "label"],
		}),
	),
	transports: destinations,
});

export default logger;

function cyan(val: string) {
	return `\x1b[36m${val}\x1b[0m`;
}

================
File: workspace/data-proxy/src/proxy-server.test.ts
================
import {
	afterAll,
	beforeAll,
	beforeEach,
	describe,
	expect,
	it,
} from "bun:test";
import { Secp256k1 } from "@cosmjs/crypto";
import { DataProxy, Environment } from "@seda-protocol/data-proxy-sdk";
import { startProxyServer } from "./proxy-server";
import {
	HttpResponse,
	registerHandler,
	server,
} from "./testutils/mock-upstream";

beforeAll(() => {
	server.listen();
});

beforeEach(() => {
	server.resetHandlers();
});

afterAll(() => {
	server.close();
});

// Data proxy setup
const privateKeyBuff = Buffer.from(new Array(32).fill(1));
const keyPair = await Secp256k1.makeKeypair(privateKeyBuff);

const dataProxy = new DataProxy(Environment.Devnet, {
	privateKey: Buffer.from(keyPair.privkey),
});

describe("proxy server", () => {
	it("should forward a body without modifying it", async () => {
		const { upstreamUrl, proxyUrl, path, port } = registerHandler(
			"post",
			"/test-post-body",
			async ({ request }) => {
				const bodyText = await request.text();
				return HttpResponse.json({ receivedBody: bodyText });
			},
		);

		const proxy = startProxyServer(
			{
				routeGroup: "",
				statusEndpoints: {
					root: "status",
				},
				routes: [
					{
						method: "POST",
						path,
						upstreamUrl,
						forwardResponseHeaders: new Set([]),
						headers: {},
					},
				],
			},
			dataProxy,
			{
				disableProof: true,
				port,
			},
		);

		const response = await fetch(proxyUrl, {
			method: "POST",
			headers: { "content-type": "application/json" },
			body: '{"key": "value"}',
		});

		const result = await response.json();
		expect(result).toEqual({
			receivedBody: '{"key": "value"}',
		});

		await proxy.stop();
	});

	it("should forward requests without params", async () => {
		const { upstreamUrl, proxyUrl, path, port } = registerHandler(
			"get",
			"/echo",
			async ({ params }) => {
				return HttpResponse.json({ receivedParams: params });
			},
		);

		const proxy = startProxyServer(
			{
				routeGroup: "",
				statusEndpoints: {
					root: "status",
				},
				routes: [
					{
						method: "GET",
						path,
						upstreamUrl,
						forwardResponseHeaders: new Set([]),
						headers: {},
					},
				],
			},
			dataProxy,
			{
				disableProof: true,
				port,
			},
		);

		const response = await fetch(proxyUrl);
		const result = await response.json();

		expect(result).toEqual({
			receivedParams: {},
		});

		await proxy.stop();
	});

	describe("status endpoints", () => {
		it("should return the status of the proxy for <statusRoot>/health", async () => {
			const { upstreamUrl, proxyUrl, path, port } = registerHandler(
				"get",
				// Empty path to make it easier to query the status endpoint
				"",
				async ({ request: { url } }) => {
					const searchparams = new URL(url).searchParams;

					if (searchparams.get("fail") === "true") {
						return HttpResponse.json({ noDataKey: "error" });
					}

					return HttpResponse.json({ data: "hello" });
				},
			);

			const proxy = startProxyServer(
				{
					routeGroup: "",
					statusEndpoints: {
						root: "status",
					},
					routes: [
						{
							method: "GET",
							path,
							upstreamUrl,
							forwardResponseHeaders: new Set([]),
							headers: {},
							jsonPath: "$.data",
						},
					],
				},
				dataProxy,
				{
					disableProof: true,
					port,
				},
			);

			async function expectStatus(expected: unknown) {
				const response = await fetch(`${proxyUrl}/status/health`);
				const result = await response.json();

				expect(result).toEqual(expected);
			}

			await expectStatus({
				status: "healthy",
				metrics: {
					uptime: expect.any(String),
					requests: 0,
					errors: 0,
				},
			});

			// Successful proxy request
			await fetch(`${proxyUrl}`);

			await expectStatus({
				status: "healthy",
				metrics: {
					uptime: expect.any(String),
					requests: 1,
					errors: 0,
				},
			});

			// Failing proxy request
			await fetch(`${proxyUrl}?fail=true`);

			await expectStatus({
				status: "healthy",
				metrics: {
					uptime: expect.any(String),
					requests: 2,
					errors: 1,
				},
			});

			await proxy.stop();
		});

		it("should return the pubkey of the proxy for <statusRoot>/pubkey", async () => {
			const { upstreamUrl, proxyUrl, path, port } = registerHandler(
				"get",
				// Empty path to make it easier to query the status endpoint
				"",
				async () => {
					return HttpResponse.json({});
				},
			);

			const proxy = startProxyServer(
				{
					routeGroup: "",
					statusEndpoints: {
						root: "status",
					},
					routes: [
						{
							method: "GET",
							path,
							upstreamUrl,
							forwardResponseHeaders: new Set([]),
							headers: {},
						},
					],
				},
				dataProxy,
				{
					disableProof: true,
					port,
				},
			);

			const response = await fetch(`${proxyUrl}/status/pubkey`);
			const result = await response.json();

			expect(result).toEqual({
				pubKey:
					"031b84c5567b126440995d3ed5aaba0565d71e1834604819ff9c17f5e9d5dd078f",
			});

			await proxy.stop();
		});

		it("should secure the status endpoint with an API key when configured", async () => {
			const { upstreamUrl, proxyUrl, path, port } = registerHandler(
				"get",
				// Empty path to make it easier to query the status endpoint
				"",
				async () => {
					return HttpResponse.json({});
				},
			);

			const proxy = startProxyServer(
				{
					routeGroup: "",
					statusEndpoints: {
						root: "status",
						apiKey: {
							header: "X-API-Key",
							secret: "secret",
						},
					},
					routes: [
						{
							method: "GET",
							path,
							upstreamUrl,
							forwardResponseHeaders: new Set([]),
							headers: {},
						},
					],
				},
				dataProxy,
				{
					disableProof: true,
					port,
				},
			);

			const unauthorizedPubkeyRes = await fetch(
				`${proxyUrl}/status/pubkey`,
			).then((r) => r.text());
			expect(unauthorizedPubkeyRes).toEqual("Unauthorized");

			const unauthorizedHealthRes = await fetch(
				`${proxyUrl}/status/health`,
			).then((r) => r.text());
			expect(unauthorizedHealthRes).toEqual("Unauthorized");

			const authorizedPubkeyRes = await fetch(`${proxyUrl}/status/pubkey`, {
				headers: {
					"X-API-Key": "secret",
				},
			}).then((r) => r.json());
			expect(authorizedPubkeyRes).toEqual({
				pubKey:
					"031b84c5567b126440995d3ed5aaba0565d71e1834604819ff9c17f5e9d5dd078f",
			});

			const authorizedHealthRes = await fetch(`${proxyUrl}/status/health`, {
				headers: {
					"X-API-Key": "secret",
				},
			}).then((r) => r.json());
			expect(authorizedHealthRes).toEqual({
				status: "healthy",
				metrics: {
					uptime: expect.any(String),
					requests: 0,
					errors: 0,
				},
			});

			await proxy.stop();
		});
	});
});

================
File: workspace/data-proxy/src/proxy-server.ts
================
import { randomUUID } from "node:crypto";
import { constants, type DataProxy } from "@seda-protocol/data-proxy-sdk";
import { tryAsync } from "@seda-protocol/utils";
import { Elysia } from "elysia";
import { Maybe } from "true-myth";
import { type Config, getHttpMethods } from "./config-parser";
import { DEFAULT_PROXY_ROUTE_GROUP, JSON_PATH_HEADER_KEY } from "./constants";
import logger from "./logger";
import { StatusContext, statusPlugin } from "./status-plugin";
import {
	createDefaultResponseHeaders,
	createSignedResponseHeaders,
} from "./utils/create-headers";
import { queryJson } from "./utils/query-json";
import { replaceParams } from "./utils/replace-params";
import { createUrlSearchParams } from "./utils/search-params";
import { injectSearchParamsInUrl } from "./utils/url";

function createErrorResponse(error: string, status: number) {
	return new Response(JSON.stringify({ data_proxy_error: error }), {
		status,
		headers: createDefaultResponseHeaders(),
	});
}

export interface ProxyServerOptions {
	port: number;
	disableProof: boolean;
}

export function startProxyServer(
	config: Config,
	dataProxy: DataProxy,
	serverOptions: ProxyServerOptions,
) {
	const server = new Elysia()
		// Assign a unique ID to every request
		.derive(() => {
			return {
				requestId: randomUUID(),
			};
		})
		.onBeforeHandle(
			({
				requestId,
				headers,
				body,
				params,
				path,
				query,
				request: { method },
			}) => {
				logger.debug("Received request", {
					requestId,
					headers,
					body,
					params,
					path,
					query,
					method,
				});
			},
		)
		.onAfterResponse(({ requestId, response }) => {
			logger.debug("Responded to request", { requestId, response });
		});

	const statusContext = new StatusContext(dataProxy.publicKey.toString("hex"));
	server.use(statusPlugin(statusContext, config.statusEndpoints));

	const proxyGroup = config.routeGroup ?? DEFAULT_PROXY_ROUTE_GROUP;

	server.group(proxyGroup, (app) => {
		// Only update the status context in routes that are part of the proxy group
		app.onBeforeHandle(() => statusContext.incrementRequests());
		app.onAfterHandle(({ response }) => {
			if (response instanceof Response && !response.ok) {
				statusContext.incrementErrors();
			}
		});

		for (const route of config.routes) {
			const routeMethods = getHttpMethods(route.method);

			// A route can have multiple methods attach to it
			for (const routeMethod of routeMethods) {
				app.route(
					routeMethod,
					route.path,
					async ({ headers, params, body, query, requestId, request }) => {
						const requestLogger = logger.child({ requestId });

						// requestBody is now always a string because of the parse function in this route
						const requestBody = Maybe.of(body as string | undefined);

						// Verification with the SEDA chain that the overlay node is eligible
						if (!serverOptions.disableProof) {
							requestLogger.debug("Verifying proof");
							const proofHeader = Maybe.of(headers[constants.PROOF_HEADER_KEY]);

							if (proofHeader.isNothing) {
								const message = `Header "${constants.PROOF_HEADER_KEY}" is not provided`;
								requestLogger.error(message);
								return createErrorResponse(message, 400);
							}

							const isValid = await dataProxy.verify(proofHeader.value);

							if (isValid.isErr || !isValid.value) {
								const message = `Invalid proof ${isValid.isErr ? isValid.error : ""}`;
								requestLogger.error(message);
								return createErrorResponse(message, 401);
							}
						} else {
							requestLogger.debug("Skipping proof verification.");
						}

						// Add the request search params (?one=two) to the upstream url
						const requestSearchParams = createUrlSearchParams(query);
						let upstreamUrl = replaceParams(route.upstreamUrl, params);
						upstreamUrl = injectSearchParamsInUrl(
							upstreamUrl,
							requestSearchParams,
						).toString();

						const upstreamHeaders = new Headers();

						// Redirect all headers given by the requester
						for (const [key, value] of Object.entries(headers)) {
							if (!value || key === constants.PROOF_HEADER_KEY) {
								continue;
							}

							upstreamHeaders.append(key, value);
						}

						// Inject all configured headers by the data proxy node configuration
						for (const [key, value] of Object.entries(route.headers)) {
							upstreamHeaders.append(key, replaceParams(value, params));
						}

						// Host doesn't match since we are proxying. Returning the upstream host while the URL does not match results
						// in the client to not return the response.
						upstreamHeaders.delete("host");

						requestLogger.debug(
							`${routeMethod} ${proxyGroup}${route.path} -> ${upstreamUrl}`,
							{ headers: upstreamHeaders, body, upstreamUrl },
						);

						const upstreamResponse = await tryAsync(async () =>
							fetch(upstreamUrl, {
								method: routeMethod,
								headers: upstreamHeaders,
								body: body as string,
							}),
						);

						if (upstreamResponse.isErr) {
							const message = `Proxying URL ${route.path} failed: ${upstreamResponse.error}`;
							requestLogger.error(message, { error: upstreamResponse.error });
							return createErrorResponse(message, 500);
						}

						requestLogger.debug("Received upstream response", {
							headers: upstreamResponse.value.headers,
						});

						const upstreamTextResponse = await tryAsync(
							async () => await upstreamResponse.value.text(),
						);

						if (upstreamTextResponse.isErr) {
							const message = `Reading ${route.path} response body failed: ${upstreamTextResponse.error}`;
							requestLogger.error(message, {
								error: upstreamTextResponse.error,
							});
							return createErrorResponse(message, 500);
						}

						let responseData: string = upstreamTextResponse.value;

						if (route.jsonPath) {
							logger.debug(`Applying route JSONpath ${route.jsonPath}`);
							const data = queryJson(
								upstreamTextResponse.value,
								route.jsonPath,
							);

							if (data.isErr) {
								requestLogger.error(
									`Failed to apply route JSONpath: ${route.jsonPath}`,
									{ error: data.error },
								);
								return createErrorResponse(data.error, 500);
							}

							responseData = JSON.stringify(data.value);
							logger.debug("Successfully applied route JSONpath");
						}

						const jsonPathRequestHeader = Maybe.of(
							headers[JSON_PATH_HEADER_KEY],
						);

						// TODO: Would be nice to only parse the JSON once
						if (jsonPathRequestHeader.isJust) {
							logger.debug(
								`Applying request JSONpath ${jsonPathRequestHeader.value}`,
							);
							// We apply the JSON path to the data that's exposed by the data proxy.
							// This allows operators to specify what data is accessible while the data request program can specify what it wants from the accessible data.
							const data = queryJson(responseData, jsonPathRequestHeader.value);

							if (data.isErr) {
								requestLogger.error(
									`Failed to apply JSONpath: ${jsonPathRequestHeader.value}`,
									{ error: data.error },
								);
								return createErrorResponse(data.error, 400);
							}

							responseData = JSON.stringify(data.value);
							logger.debug("Successfully applied request JSONpath");
						}

						const signature = await dataProxy.signData(
							request.url,
							request.method,
							Buffer.from(requestBody.isJust ? requestBody.value : "", "utf-8"),
							Buffer.from(responseData, "utf-8"),
						);

						const responseHeaders = new Headers();

						// Forward all headers that are configured in the config.json
						for (const forwardHeaderKey of route.forwardResponseHeaders) {
							const forwardHeaderValue =
								upstreamResponse.value.headers.get(forwardHeaderKey);

							if (forwardHeaderValue) {
								responseHeaders.append(forwardHeaderKey, forwardHeaderValue);
							}
						}

						return new Response(responseData, {
							headers: createSignedResponseHeaders(signature, responseHeaders),
						});
					},
					{
						config: {},
						parse: ({ request }) => {
							// TODO: forward the request body transparently.
							// https://github.com/sedaprotocol/seda-data-proxy/issues/12
							return request.text();
						},
					},
				);
			}
		}

		return app;
	});

	server.listen(serverOptions.port);
	logger.info(
		`Proxy routes is at http://127.0.0.1:${serverOptions.port}/${proxyGroup === "" || proxyGroup.endsWith("/") ? `${proxyGroup}` : `${proxyGroup}/`}`,
	);

	return server;
}

================
File: workspace/data-proxy/config.example.json
================
{
	"routes": [
		{
			"path": "/api/*",
			"upstreamUrl": "https://swapi.dev/api/{*}",
			"headers": {
				"x-api-token": "secret"
			}
		}
	]
}

================
File: workspace/data-proxy/package.json
================
{
	"name": "@seda-protocol/data-proxy",
	"main": "./src/index.ts",
	"version": "0.0.3",
	"devDependencies": {
		"@types/big.js": "^6.2.2",
		"msw": "^2.3.5"
	},
	"dependencies": {
		"@commander-js/extra-typings": "^12.1.0",
		"@cosmjs/crypto": "^0.32.4",
		"@seda-protocol/data-proxy-sdk": "workspace:*",
		"@seda-protocol/utils": "^1.0.0",
		"big.js": "6.2.1",
		"commander": "^12.1.0",
		"date-fns": "^4.1.0",
		"elysia": "^1.1.6",
		"jsonpath-plus": "^9.0.0",
		"secp256k1": "^5.0.0",
		"true-myth": "^8.0.0",
		"valibot": "0.37.0",
		"winston": "^3.14.2",
		"winston-daily-rotate-file": "^5.0.0"
	}
}

================
File: workspace/data-proxy-sdk/src/config.ts
================
export enum Environment {
	Mainnet = "mainnet",
	Testnet = "testnet",
	Devnet = "devnet",
}

export interface DataProxyOptions {
	rpcUrl: string;

	// URL to the explorer page
	explorerUrl: string;

	privateKey: Buffer;

	coreContract?: string;
}

export const defaultConfig: Record<Environment, DataProxyOptions> = {
	mainnet: {
		rpcUrl: "https://rpc.seda.xyz",
		explorerUrl: "https://explorer.seda.xyz",
		privateKey: Buffer.from([]),
	},
	testnet: {
		rpcUrl: "https://rpc.testnet.seda.xyz",
		explorerUrl: "https://testnet.explorer.seda.xyz",
		privateKey: Buffer.from([]),
	},
	devnet: {
		rpcUrl: "https://rpc.devnet.seda.xyz",
		explorerUrl: "https://devnet.explorer.seda.xyz",
		privateKey: Buffer.from([]),
	},
};

================
File: workspace/data-proxy-sdk/src/constants.ts
================
export const PROOF_HEADER_KEY = "x-seda-proof";
export const SIGNATURE_HEADER_KEY = "x-seda-signature";
export const PUBLIC_KEY_HEADER_KEY = "x-seda-publickey";
export const SIGNATURE_VERSION_HEADER_KEY = "x-seda-version";

================
File: workspace/data-proxy-sdk/src/data-proxy.test.ts
================
import { describe, expect, it } from "bun:test";
import { Secp256k1 } from "@cosmjs/crypto";
import { Environment } from "./config";
import { DataProxy } from "./data-proxy";

describe("DataProxy", async () => {
	const privateKeyBuff = Buffer.from(new Array(32).fill(1));
	const keyPair = await Secp256k1.makeKeypair(privateKeyBuff);

	const dataProxy = new DataProxy(Environment.Devnet, {
		privateKey: Buffer.from(keyPair.privkey),
	});

	describe("signData", () => {
		it("should sign valid data", async () => {
			const signature = await dataProxy.signData(
				"https://example.com",
				"get",
				Buffer.from([]),
				Buffer.from(
					JSON.stringify({
						name: "data-proxy",
					}),
				),
			);

			expect(signature.publicKey).toBe(
				"031b84c5567b126440995d3ed5aaba0565d71e1834604819ff9c17f5e9d5dd078f",
			);
			expect(signature.signature).toBe(
				"c3e4f2b4c73612ae2da70fa0377b02b107a54d3f7ab9dd74e83f7563eeaf2a5d31ee5a8fe3be64f3fc60ad22c237677091fce1d61a3ba434215e0aac13426d40",
			);
		});
	});

	describe("hashInputs", () => {
		it("should hash and concatenate the inputs", () => {
			const message = dataProxy.generateMessage(
				"https://example.com",
				"get",
				Buffer.from([]),
				Buffer.from(
					JSON.stringify({
						name: "data-proxy",
					}),
				),
			);
			expect(Buffer.from(message).toString("hex")).toBe(
				"edba3f8cfcd4165f73cd4641ced2b2ec0d3ba4338e3eec30edd58777d86b53b25a61babeb76c554783ca90a1a250e84f1b703409fdff33c217ab64dd51f05199c5d2460186f7233c927e7db2dcc703c0e500b653ca82273b7bfad8045d85a4706db57ed7cc68d9897b06df02ed002ce206633eec05690d504d61789ae87db019",
			);
		});
	});
});

================
File: workspace/data-proxy-sdk/src/data-proxy.ts
================
import { CosmWasmClient } from "@cosmjs/cosmwasm-stargate";
import { keccak256 } from "@cosmjs/crypto";
import { tryAsync } from "@seda-protocol/utils";
import { ecdsaSign, publicKeyCreate } from "secp256k1";
import { Maybe, Result } from "true-myth";
import {
	type DataProxyOptions,
	type Environment,
	defaultConfig,
} from "./config";
import { getLatestCoreContractAddress } from "./latest-core-contract-address";

export interface SignedData {
	// Hex encoded signature
	signature: string;

	// Hex encoded public key
	publicKey: string;

	// Signature recover id
	recId: number;

	// Version of the signature
	version: string;
}

export class DataProxy {
	private version = "0.1.0";
	public publicKey: Buffer;
	private privateKey: Buffer;
	public options: DataProxyOptions;
	private cosmwasmClient: Maybe<CosmWasmClient> = Maybe.nothing();
	private coreContractAddress: Maybe<string> = Maybe.nothing();

	constructor(
		public environment: Environment,
		optionsOverride: Partial<DataProxyOptions> = {},
	) {
		// Remove undefined variables, so that the default config can override them
		for (const optionKey of Object.keys(optionsOverride)) {
			const key = optionKey as keyof DataProxyOptions;

			if (typeof optionsOverride[key] === "undefined") {
				delete optionsOverride[key];
			}
		}

		this.options = {
			...defaultConfig[environment],
			...optionsOverride,
		};

		this.privateKey = this.options.privateKey;
		this.publicKey = Buffer.from(publicKeyCreate(this.privateKey, true));

		if (this.options.coreContract) {
			this.coreContractAddress = Maybe.just(this.options.coreContract);
		}

		// Trigger fetching of client and address
		this.getCosmWasmClient();
		this.getCoreContractAddress();
	}

	private async getCosmWasmClient(): Promise<Result<CosmWasmClient, unknown>> {
		if (this.cosmwasmClient.isNothing) {
			const client = await tryAsync(async () =>
				CosmWasmClient.connect(this.options.rpcUrl),
			);

			if (client.isOk) {
				this.cosmwasmClient = Maybe.just(client.value);
				return Result.ok(client.value);
			}

			return client;
		}

		return Result.ok(this.cosmwasmClient.value);
	}

	private async getCoreContractAddress(): Promise<Result<string, unknown>> {
		if (this.coreContractAddress.isNothing) {
			const address = await getLatestCoreContractAddress(this.options.rpcUrl);

			if (address.isOk) {
				this.coreContractAddress = Maybe.just(address.value);
				return Result.ok(address.value);
			}

			return address;
		}

		return Result.ok(this.coreContractAddress.value);
	}

	/**
	 * Verifies if the executor is eligible or not
	 * proof is given by the executor through the header x-proof
	 * @param payload
	 */
	async verify(proof: string): Promise<Result<boolean, string>> {
		// Verify if eligible (right now is this one staked or not)
		const client = await this.getCosmWasmClient();
		if (client.isErr) {
			return Result.err(`Could not create client ${client.error}`);
		}

		const coreContractAddress = await this.getCoreContractAddress();
		if (coreContractAddress.isErr) {
			return Result.err(
				`Could not get contract address ${coreContractAddress.error}`,
			);
		}

		const result = await tryAsync(async () =>
			client.value.queryContractSmart(coreContractAddress.value, {
				is_executor_eligible: {
					data: proof,
				},
			}),
		);

		return result.mapErr((err) => `Error while fetching verification: ${err}`);
	}

	/**
	 * Signs data and gives back a wrapped signed response
	 *
	 * @param data
	 */
	async signData(
		requestUrl: string,
		requestMethod: string,
		requestBody: Buffer,
		responseBody: Buffer,
	): Promise<SignedData> {
		const signResult = this.hashAndSign(
			this.generateMessage(
				requestUrl,
				requestMethod,
				requestBody,
				responseBody,
			),
		);

		return {
			publicKey: this.publicKey.toString("hex"),
			signature: Buffer.from(signResult.signature).toString("hex"),
			recId: signResult.recid,
			version: this.version,
		};
	}

	generateMessage(
		requestUrl: string,
		requestMethod: string,
		requestBody: Buffer,
		responseBody: Buffer,
	) {
		const requestUrlHash = keccak256(Buffer.from(requestUrl));
		const requestMethodHash = keccak256(
			Buffer.from(requestMethod.toUpperCase()),
		);
		const requestBodyHash = keccak256(requestBody);
		const responseBodyHash = keccak256(responseBody);

		return Buffer.concat([
			requestUrlHash,
			requestMethodHash,
			requestBodyHash,
			responseBodyHash,
		]);
	}

	hashAndSign(message: Buffer) {
		return ecdsaSign(keccak256(message), this.privateKey);
	}
}

================
File: workspace/data-proxy-sdk/src/index.ts
================
export { DataProxy, type SignedData } from "./data-proxy";
export { type DataProxyOptions, Environment } from "./config";
export * as constants from "./constants";

================
File: workspace/data-proxy-sdk/src/latest-core-contract-address.ts
================
import { QueryClient, createProtobufRpcClient } from "@cosmjs/stargate";
import { Comet38Client } from "@cosmjs/tendermint-rpc";
import { sedachain } from "@seda-protocol/proto-messages";
import { tryAsync } from "@seda-protocol/utils";
import { Result } from "true-myth";

export async function getLatestCoreContractAddress(
	rpc: string,
): Promise<Result<string, unknown>> {
	const cometClient = await tryAsync(async () => Comet38Client.connect(rpc));

	if (cometClient.isErr) {
		return Result.err(cometClient.error);
	}

	const queryClient = new QueryClient(cometClient.value);
	const protoRpcClient = createProtobufRpcClient(queryClient);

	const sedaQueryClient = new sedachain.wasm_storage.v1.QueryClientImpl(
		protoRpcClient,
	);
	const response = await tryAsync(async () =>
		sedaQueryClient.CoreContractRegistry({}),
	);

	return response.map((v) => v.address);
}

================
File: workspace/data-proxy-sdk/package.json
================
{
	"name": "@seda-protocol/data-proxy-sdk",
	"main": "./src/index.ts",
	"type": "module",
	"dependencies": {
		"@cosmjs/cosmwasm-stargate": "^0.32.4",
		"@cosmjs/crypto": "^0.32.4",
		"@seda-protocol/proto-messages": "0.3.0-dev.0-1",
		"@seda-protocol/utils": "^1.0.0",
		"secp256k1": "^5.0.0",
		"true-myth": "^8.0.0"
	}
}

================
File: .dockerignore
================
config.json
data-proxy-private-key.json

================
File: .env.example
================
SEDA_DATA_PROXY_PRIVATE_KEY=

================
File: .gitignore
================
# Based on https://raw.githubusercontent.com/github/gitignore/main/Node.gitignore

# Logs

logs
_.log
npm-debug.log_
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Caches

.cache

# Diagnostic reports (https://nodejs.org/api/report.html)

report.[0-9]_.[0-9]_.[0-9]_.[0-9]_.json

# Runtime data

pids
_.pid
_.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover

lib-cov

# Coverage directory used by tools like istanbul

coverage
*.lcov

# nyc test coverage

.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)

.grunt

# Bower dependency directory (https://bower.io/)

bower_components

# node-waf configuration

.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)

build/Release

# Dependency directories

node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)

web_modules/

# TypeScript cache

*.tsbuildinfo

# Optional npm cache directory

.npm

# Optional eslint cache

.eslintcache

# Optional stylelint cache

.stylelintcache

# Microbundle cache

.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history

.node_repl_history

# Output of 'npm pack'

*.tgz

# Yarn Integrity file

.yarn-integrity

# dotenv environment variable files

.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)

.parcel-cache

# Next.js build output

.next
out

# Nuxt.js build / generate output

.nuxt
dist

# Gatsby files

# Comment in the public line in if your project uses Gatsby and not Next.js

# https://nextjs.org/blog/next-9-1#public-directory-support

# public

# vuepress build output

.vuepress/dist

# vuepress v2.x temp and cache directory

.temp

# Docusaurus cache and generated files

.docusaurus

# Serverless directories

.serverless/

# FuseBox cache

.fusebox/

# DynamoDB Local files

.dynamodb/

# TernJS port file

.tern-port

# Stores VSCode versions used for testing VSCode extensions

.vscode-test

# yarn v2

.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*

# IntelliJ based IDEs
.idea

# Finder (MacOS) folder config
.DS_Store
config.json
data-proxy-private-key.json

ubuntu-dind*

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.6.0
    hooks:
      - id: trailing-whitespace
        exclude: \.md$
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

================
File: biome.json
================
{
	"$schema": "https://biomejs.dev/schemas/1.8.3/schema.json",
	"files": {
		"ignore": ["./workspace/data-proxy-sdk/gen"]
	},
	"organizeImports": {
		"enabled": true
	},
	"linter": {
		"enabled": true,
		"rules": {
			"recommended": true
		}
	}
}

================
File: bunfig.toml
================
[test]
preload = ["./test-setup.ts"]

================
File: Makefile
================
.PHONY: build run stop up clean logs ssh

# Define the docker-compose file location
DOCKER_COMPOSE_FILE := .build/docker/docker-compose.yml

# Build the Docker image
build:
	docker compose -f $(DOCKER_COMPOSE_FILE) build

# Run the Docker container
run:
	docker compose -f $(DOCKER_COMPOSE_FILE) up

# Stop the Docker container
stop:
	docker compose -f $(DOCKER_COMPOSE_FILE) down

# Build and run the Docker container
up: build run

# Clean up Docker resources
clean:
	docker compose -f $(DOCKER_COMPOSE_FILE) down --rmi all --volumes --remove-orphans

# Show logs
logs:
	docker compose -f $(DOCKER_COMPOSE_FILE) logs -f

# SSH into the running container
ssh:
	docker compose -f $(DOCKER_COMPOSE_FILE) exec seda-data-proxy sh

================
File: package.json
================
{
	"name": "seda-data-proxy",
	"module": "./src/index.ts",
	"main": "./src/index.ts",
	"type": "module",
	"scripts": {
		"start": "bun run ./workspace/data-proxy",
		"fmt": "bunx biome check --write .",
		"check-fmt": "bunx biome check ."
	},
	"devDependencies": {
		"@biomejs/biome": "1.8.3",
		"@types/bun": "latest",
		"@types/secp256k1": "^4.0.6"
	},
	"workspaces": ["workspace/*"],
	"peerDependencies": {
		"typescript": "^5.0.0"
	}
}

================
File: Procfile
================
web: bun start init && bun start run --disable-proof --config ./config.json

================
File: README.md
================
# SEDA Data Proxy

Allows Data Providers to expose their (private) APIs on the SEDA network. Only eligible overlay nodes are allowed to access the proxy.

## Set up

Install bun:

```sh
curl -fsSL https://bun.sh/install | bash
```

Install all project dependencies:

```sh
bun install
```

Now you are able to run the Data Proxy CLI:

```sh
bun start --help
```

## Running a node

Run the init command to create a keypair and an example config:

```sh
bun start init
```

This will generate two files:

- `config.json`: Configure where routes are going to and what to inject (ex: headers)
- `data-proxy-private-key.json`: Private key that signs the HTTP response. This key is registered on the SEDA chain (see below). If required you can also use the `SEDA_DATA_PROXY_PRIVATE_KEY` environment variable to expose the private key to the node.

Now you can run the node:

```sh
# Disables the proofing mechanism so it's easier to debug the proxy
bun start run --disable-proof

# The console will output something similiar:
2024-08-19 13:21:46.624 info: Proxy routes is at http://127.0.0.1:5384/proxy/
```

Now you can access the SWApi through curl, browser, or any other HTTP client:

```sh
curl http://localhost:5384/proxy/planets/1
```

The node will auto sign the response and include two headers: `x-seda-signature` and `x-seda-publickey`, which will be used for verification on the executor node.

## Proxy rules

- Request query params are forwared to the `upstreamUrl`
- Request headers except `host` are forwared to the `upstreamUrl`
- Request Body is forwared to the `upstreamUrl`
- By default only the upstream header `content-type` is given back. This can however be configured to include more.
- The full body is given back as a response. This can be reduced with using `jsonPath`

## Configuration

The config file allows you to configure multiple routes:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      "path": "/eth-usd",
      "upstreamUrl": "https://myapi.com/eth-usd",
      // Default is GET
      "headers": {
        "x-api-key": "some-api-key"
      }
    },
    {
      "path": "/btc-usd",
      "upstreamUrl": "https://myapi.com/btc-usd",
      // Allows for multiple method setting
      "method": ["GET", "HEAD"],
      "headers": {
        "x-api-key": "some-api-key"
      }
    }
  ]
}
```

### Variables

The config.json has support for using variable routes by using `:varName`:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      "path": "/:coinA/:coinB",
      // Use {} to inject route variables
      "upstreamUrl": "https://myapi.com/{:coinA}-{:coinB}",
      "headers": {
        "x-api-key": "some-api-key",
        // Can also be injected in the header
        "x-custom": "{:coinA}"
      }
    }
  ]
}
```

### JSON Path

If you don't want to expose all API info you can use `jsonPath` to reduce the response:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      "path": "/planets/:planet",
      "upstreamUrl": "https://swapi.dev/api/planets/{:planet}",
      // Calling the API http://localhost:5384/proxy/planets/1 will only return "Tatooine" and omit the rest
      "jsonPath": "$.name",
      "headers": {
        "x-api-key": "some-api-key"
      }
    }
  ]
}
```

### Forwarding headers

By default the data proxy node will only forward the `content-type` as a response. This can be configured to include more headers if desired:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      "path": "/planets/:planet",
      "upstreamUrl": "https://swapi.dev/api/planets/{:planet}",
      // Now the API will also return the server header from SWApi
      "forwardResponseHeaders": ["content-type", "server"],
      "headers": {
        "x-api-key": "some-api-key"
      }
    }
  ]
}
```

### Environment variables injection

Sometimes you don't want to expose your API key in a config file, or you have multiple environments running. The Data Proxy node has support for injecting environment variables through `{$MY_ENV_VARIABLE}`:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      // Everything will be injected in the URL
      "path": "/*",
      "upstreamUrl": "https://swapi.dev/api/{*}",
      "headers": {
        "x-api-key": "{$SECRET_API_KEY}"
      }
    }
  ]
}
```

### Wildcard routes

The Data Proxy node has support for wildcard routes, which allows you to quickly expose all your APIs:

```jsonc
{
  "routeGroup": "proxy",
  "routes": [
    {
      // Everything will be injected in the URL
      "path": "/*",
      "upstreamUrl": "https://swapi.dev/api/{*}",
      "headers": {
        "x-api-key": "some-api-key"
      }
    }
  ]
}
```

## Status endpoints

The Data Proxy node has support for exposing status information through some endpoints. This can be used to monitor the health of the node and the number of requests it has processed.

The status endpoint has two routes:

- `/<statusEndpointsRoot>/health`  
  Returns a JSON object with the following structure:

  ```json
  {
    "status": "healthy",
    "metrics": {
      "uptime": "P0Y0M1DT2H3M4S", // ISO 8601 duration since the node was started
      "requests": 1024, // Number of requests processed
      "errors": 13 // Number of errors that occurred
    }
  }
  ```

- `/<statusEndpointsRoot>/pubkey`  
  Returns the public key of the node.
  ```json
  {
    "pubkey": "031b84c5567b126440995d3ed5aaba0565d71e1834604819ff9c17f5e9d5dd078f"
  }
  ```

### Configuration

The status endpoints can be configured in the config file:

```json
{
  // Other config...
  "statusEndpoints": {
    "root": "status",
    // Optional
    "apiKey": {
      "header": "x-api-key",
      "secret": "some-secret"
    }
  }
}
```

- `root`: Root path for the status endpoints. Defaults to `status`.
- `apiKey`: Optionally secure the status endpoints with an API key. The `header` attribute is the header key that needs to be set, and `secret` is the value that it needs to be set to.  
  The `statusEndpoints.apiKey.secret` attribute supports the `{$MY_ENV_VARIABLE}` syntax for injecting a value from the environment during start up.

================
File: test-setup.ts
================
import "@seda-protocol/utils/testing";

================
File: tsconfig.json
================
{
	"compilerOptions": {
		// Enable latest features
		"lib": ["ESNext", "DOM"],
		"target": "ESNext",
		"module": "ESNext",
		"moduleDetection": "force",
		"jsx": "react-jsx",
		"allowJs": true,

		// Bundler mode
		"moduleResolution": "bundler",
		"allowImportingTsExtensions": true,
		"verbatimModuleSyntax": true,
		"noEmit": true,

		// Best practices
		"strict": true,
		"skipLibCheck": true,
		"noFallthroughCasesInSwitch": true,

		// Some stricter flags (disabled by default)
		"noUnusedLocals": false,
		"noUnusedParameters": false,
		"noPropertyAccessFromIndexSignature": false
	}
}
